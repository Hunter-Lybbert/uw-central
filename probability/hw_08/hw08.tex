\documentclass[10pt]{amsart}
%\include{amsmath}
\usepackage{mathtools}
\usepackage{amsmath}  
\usepackage{amssymb}  % gives you \mathbb{} font
\usepackage{dsfont}	% gives you \mathds{} font

%                   Math Blackboard Bold Symbols

\newcommand\Cb{\mathds{C}}
\newcommand\Eb{\mathds{E}}
\newcommand\Fb{\mathds{F}}
\newcommand\Gb{\mathds{G}}
\newcommand\Ib{\mathds{I}}
\newcommand\Pb{\mathds{P}}
\newcommand\Qb{\mathds{Q}}
\newcommand\Rb{\mathds{R}}
%\newcommand\Zb{\mathds{Z}}
\newcommand\Nb{\mathds{N}}
\newcommand\Vb{\mathds{V}}
\newcommand\Ub{\mathds{U}}

\usepackage[shortlabels]{enumitem}
\usepackage{amssymb}
\usepackage{bbm}
\usepackage{bm}
\usepackage{cancel}
\usepackage{graphicx,subfig}

\graphicspath{ {./images/} }

\newcommand{\D}{\mathrm{d}}
\DeclareMathOperator{\E}{e}
\DeclareMathOperator{\I}{i}


\begin{document}

\noindent
\text{Hunter Lybbert} \\
\text{Student ID: 2426454} \\
\text{12-04-24} \\
\text{AMATH 561}
\title{Problem Set 8}
\maketitle

{\it Note: Exercises are from Matt Lorig's notes (link on course website).} \\

\noindent {\bf 1.} Exercise 5.1.
Patients arrive at an emergency room as a Poisson process with intensity $\lambda$.
The time to treat each patient is an independent exponential random variable with parameter $\mu$.
Let $X = (X_t)_{t \geq 0}$ be the number of patients in the system (either being treated or waiting).
Write down the generator of $X$.
Show that $X$ has an invariant distribution $\bm \pi $ if and only if $\lambda < \mu$.
Find $\bm \pi $.
What is the total expected time (waiting + treatment) a patient waits when the system is in its invariant distribution? \\

\noindent
Hint: You can use Little's law, which states that the expected number of people in the hospital at steady-state is equal to the average arrival rate multiplied by the average processing time. \\

\noindent
\textit{Solution:} \\
The generator for $X$ is
\begin{align*}
\bm G = 
\begin{bmatrix}
- \lambda & \lambda & 0 & 0 & 0 & \dots \\
\mu & - (\mu + \lambda) & \lambda & 0 & 0 & \dots \\
0 & \mu & - (\mu + \lambda) & \lambda & 0 & \dots \\
0 & 0 & \mu & - (\mu + \lambda) & \lambda & \ddots \\
0 & 0 & 0 & \mu & - (\mu + \lambda) & \ddots \\
\vdots & \vdots & \vdots & \ddots & \ddots & \ddots \\
\end{bmatrix}.
\end{align*}
Now if the invariant distribution $\bm \pi$ exists then $ \bm \pi \bm G = \bm 0$.
Let's look at what conditions would need to hold for this $\bm \pi$ to exist.
First, looking at $\bm \pi \bm G$ we have
\begin{align*}
0 &= - \lambda \pi(0) + \mu \pi(1) \\
0 &= \lambda \pi(0) - (\mu + \lambda) \pi(1) + \mu \pi(2) \\
0 &= \lambda \pi(1) - (\mu + \lambda) \pi(2) + \mu \pi(3) \\
& \vdots \\
0 &= \lambda \pi(n - 1) - (\mu + \lambda) \pi(n) + \mu \pi(n + 1).
\end{align*}
Then we can say
\begin{align*}
\pi(1) = \frac {\lambda}{\mu }\pi(0), \quad
\pi(2) = \frac {\lambda^2}{\mu^2 }\pi(0), \quad \dots, \quad
\pi(n) = \frac {\lambda^n}{\mu^n }\pi(0).
\end{align*}
If $\bm \pi$ is a stationary distribution then the row vector needs to sum to one so we have the condition
$$
\sum_{n=0}^{\infty} \pi(0) \left(\frac {\lambda}{\mu}\right)^n = 1.
$$
This sum is finite if and only if
$$
\left| \frac \lambda \mu \right| < 1 \implies \lambda < \mu.
$$
Furthermore, we have
\begin{align*}
\sum_{n=0}^{\infty} \pi(0) \left(\frac {\lambda}{\mu}\right)^n &= 1 \\
\pi(0) \frac 1 {1 - \frac \lambda \mu} &= 1 \\
\pi(0) &= 1 - \frac \lambda \mu.
\end{align*}
Hence,
$$
\pi(n) = \left( 1 - \frac \lambda \mu \right) \left(\frac {\lambda}{\mu}\right)^n.
$$
Therefore, we have found the stationary distribution $\bm \pi$ which only exists if and only if the condition that $\lambda < \mu$ since the sum of the entries of the vector $\bm \pi$ is only finite in this scenario. \\

\noindent
Now I need to find the total expected time (waiting + treatment) a patient waits when the system is in its invariant distribution using Little's law which gives
$$
E \left( X_t \right) = \lambda E \left( \tau \right).
$$
Then we have
\begin{align*}
E \left( \tau \right) &= \frac 1 \lambda E \left( X_t \right) \\
	&= \frac 1 \lambda \sum_{n = 0}^\infty n \pi(n) \\
	&= \frac 1 \lambda \sum_{n = 0}^\infty n \left( 1 - \frac \lambda \mu \right) \left( \frac \lambda \mu \right)^n \\
	&= \frac 1 \lambda \left( 1 - \frac \lambda \mu \right) \sum_{n = 0}^\infty n \left( \frac \lambda \mu \right)^n \\
	&= \frac 1 \lambda \left( 1 - \frac \lambda \mu \right) \frac {\frac \lambda \mu}{\left( 1 - \frac \lambda \mu \right)^2} \\
	&= \frac 1 \lambda \frac {\frac \lambda \mu}{1 - \frac \lambda \mu} \\
	&= \frac {\frac 1 \mu}{1 - \frac \lambda \mu} \\
	&= \frac 1 {\mu - \lambda}.
\end{align*}
Therefore, the expected time (waiting + treatment) a patient waits when the system is in its invariant distribution is $1/(\mu - \lambda)$. \\
\qed \\

\newpage


\noindent {\bf 2.} Exercise 5.3.
Let $X = (X_t)_{t \geq 0}$ be a Markov chain with state space $S = \{0, 1, 2, \dots\}$ and with a generator ${ \bf G} $ whose $i$th row has entries
$$
g_{i, i-1} = i\mu, \quad g_{i, i} = -i \mu - \lambda, \quad g_{i, i+1} = \lambda,
$$
with all other entries being zero (the zeroth row has only two entries: $g_{0,0}$ and $g_{0,1}$).
Assume $X_0 = j$.
Find $G_{X_T}(s) := E(s^{X_t})$.
What is the distribution of $X_t$ as $t \rightarrow \infty$? \\

\noindent
\textit{Solution:} \\
For my sake I am going to write down the generator $\bm G$
\begin{align*}
\bm G = 
\begin{bmatrix}
- \lambda & \lambda & 0 & 0 & \dots \\
\mu & - (\mu + \lambda) & \lambda & 0 & \dots \\
0 & 2\mu & - (2\mu + \lambda) & \lambda & \ddots \\
0 & 0 & 3\mu & - (3\mu + \lambda) & \ddots \\
\vdots & \vdots & \ddots & \ddots & \ddots
\end{bmatrix}.
\end{align*}
Then we want to calculate $ G_{X_T}(s) := E(s^{X_t}) $.
This takes the form
\begin{align*}
E(s^{X_t}) = \sum_{n=0}^\infty s^{n} P(X_t = n),
\end{align*}
but we need to use the Kolmogorov forward equation to find these $P(X_t = n)$ terms.
The forward equation is
$$ \frac \D {\D t} \bm P_t = \bm P_t \bm G $$
which more explicitly is
\begin{align*}
\begin{bmatrix}
p^\prime_t(0,0) & p^\prime_t(0,1) & p^\prime_t(0,2) & \dots \\
p^\prime_t(1,0) & p^\prime_t(1,1) & p^\prime_t(1,2) & \dots \\
p^\prime_t(2,0) & p^\prime_t(2,1) & p^\prime_t(2,2) & \dots \\
\vdots & \vdots & \vdots & \ddots
\end{bmatrix}
=
\begin{bmatrix}
p_t(0,0) & p_t(0,1) & p_t(0,2) & \dots \\
p_t(1,0) & p_t(1,1) & p_t(1,2) & \dots \\
p_t(2,0) & p_t(2,1) & p_t(2,2) & \dots \\
\vdots & \vdots & \vdots & \ddots
\end{bmatrix}
\begin{bmatrix}
- \lambda & \lambda & 0 & \dots \\
\mu & - (\mu + \lambda) & \lambda & \ddots \\
0 & 2\mu & - (2\mu + \lambda) & \ddots \\
\vdots & \ddots & \ddots & \ddots
\end{bmatrix}.
\end{align*}
This results in the following system of differential equations
\begin{align*}
p^\prime_t(0,0) &= -\lambda p_t(0,0) + \mu p_t(0,1) \\
p^\prime_t(0,1) &= \lambda p_t(0,0) - (\mu + \lambda) p_t(0,1) + 2\mu p_t(0,2) \\
p^\prime_t(0,2) &= \lambda p_t(0,1) - (2\mu + \lambda) p_t(0,2) + 3\mu p_t(0,3) \\
& \vdots \\
p^\prime_t(0,n) &= \lambda p_t(0,n-1) - (n\mu + \lambda) p_t(0,n) + (n + 1)\mu p_t(0,n+1).
\end{align*}
As it turns out this is true for any starting point instead of just 0, therefore, we have
\begin{align*}
p^\prime_t(j,0) &= -\lambda p_t(j,0) + \mu p_t(j,1) \\
p^\prime_t(j,1) &= \lambda p_t(j,0) - (\mu + \lambda) p_t(j,1) + 2\mu p_t(j,2) \\
p^\prime_t(j,2) &= \lambda p_t(j,1) - (2\mu + \lambda) p_t(j,2) + 3\mu p_t(j,3) \\
& \vdots \\
p^\prime_t(j,n) &= \lambda p_t(j,n-1) - (n\mu + \lambda) p_t(j,n) + (n + 1)\mu p_t(j,n+1).
\end{align*}
Multiplying through by $s^n$ and summing over all $n$ we have
\begin{align*}
\sum_{n=0}^\infty s^n p^\prime_t(j,n)
	&= \lambda \sum_{n=0}^\infty s^n p_t(j,n-1) - \sum_{n=0}^\infty s^n  (n\mu + \lambda) p_t(j,n) + \sum_{n=0}^\infty s^n (n + 1)\mu p_t(j,n+1) \\
\frac {\partial G_{X_T}(s)}{\partial t}
	&= \lambda s \sum_{n=0}^\infty s^{n - 1} p_t(j,n-1) - \sum_{n=0}^\infty s^n  (n\mu + \lambda) p_t(j,n) + \mu \frac {\partial G_{X_T}(s)}{\partial s} \\
\frac {\partial G_{X_T}(s)}{\partial t}
	&= \lambda s G_{X_T}(s) - \sum_{n=0}^\infty s^n  (n\mu + \lambda) p_t(j,n) + \mu \frac {\partial G_{X_T}(s)}{\partial s} \\
\frac {\partial G_{X_T}(s)}{\partial t}
	&= \lambda s G_{X_T}(s) - \sum_{n=0}^\infty s^n n\mu p_t(j,n) - \sum_{n=0}^\infty s^n \lambda p_t(j,n) + \mu \frac {\partial G_{X_T}(s)}{\partial s} \\
\frac {\partial G_{X_T}(s)}{\partial t}
	&= \lambda s G_{X_T}(s) - \mu s \sum_{n=0}^\infty ns^{n-1} p_t(j,n) - \lambda G_{X_T}(s) + \mu \frac {\partial G_{X_T}(s)}{\partial s} \\
\frac {\partial G_{X_T}(s)}{\partial t}
	&= \lambda s G_{X_T}(s) - \mu s \frac {\partial G_{X_T}(s)}{\partial s} - \lambda G_{X_T}(s) + \mu \frac {\partial G_{X_T}(s)}{\partial s} \\
\frac {\partial G_{X_T}(s)}{\partial t}
	&= (s - 1) \lambda G_{X_T}(s) + (1 - s)\mu \frac {\partial }{\partial s}G_{X_T}(s).
\end{align*}
Suppressing some notation for convenience, we now need to solve the following differential equation with initial condition of $G_{X_0}(s) = s^j$
$$
\frac {\partial}{\partial t} G
	= (s - 1) \lambda G + (1 - s)\mu \frac {\partial }{\partial s}G.
$$
Using mathematica to solve this pde I ended up with
$$
G_{X_t}(s) = \exp\left( \frac {\E^{-t \mu} (-1 + \E^{t \mu}) (-1 + s) \lambda } {\mu}  \right) \left( 1 + \E^{-t \mu} (-1 + s)) \right)^j.
$$
Simplifying a little we have
$$
G_{X_t}(s) = \exp\left( \frac {(1 - \E^{-t \mu}) (s - 1) \lambda } {\mu}  \right) \left( 1 + \E^{-t \mu} (s - 1) \right)^j.
$$
The generating function as 
\begin{align*}
\lim_{t \rightarrow \infty} G_{X_t}(s)
	&= \lim_{t \rightarrow \infty} \exp \left( \frac {(1 - \E^{-t \mu}) (s - 1) \lambda } {\mu}  \right) \left( 1 + \E^{-t \mu} (s - 1) \right)^j \\
	&= \exp\left( \frac {(1 - 0) (s - 1) \lambda } {\mu}  \right) \left( 1 + 0 (s - 1) \right)^j \\
	&= \exp\left( \frac {(s - 1) \lambda } {\mu}  \right).
\end{align*}
Let $G_X(s) = \exp\left( \frac {(s - 1) \lambda } {\mu}  \right)$ denote the generating function after taking the limit as $t$ goes to infinity.
We can compute the distribution from the generating function as
\begin{align*}
p_n(t) &= \left. \frac 1 {n!} \frac {\partial^n} {\partial s^n}G_X(s) \right|_{s=0}\\
	&= \left. \frac 1 {n!} \frac {\partial^n} {\partial s^n} \exp\left( \frac {(s - 1) \lambda } {\mu}  \right) \right|_{s=0} \\
	&= \left. \frac 1 {n!} \left( \frac \lambda \mu \right)^n \exp\left( \frac {(s - 1) \lambda } {\mu}  \right) \right|_{s=0} \\
	&= \frac 1 {n!} \left( \frac \lambda \mu \right)^n \exp\left( - \frac \lambda \mu \right) \\
	&= \frac 1 {n!} \left( \frac \lambda \mu \right)^n \E^{- \frac \lambda \mu}.
\end{align*}
Which is a poisson distribution with rate $\lambda / \mu$. \\
\qed \\

\newpage

\noindent {\bf 3.} Exercise 5.4.
Let $N$ be a time-inhomogeneous Poisson process with intensity function $\lambda(t)$.
That is, the probability of a jump of size one in the time interval $(t, t + \D t$) is $\lambda(t) \D t$ and the probability of two jumps in that interval of time is $\mathcal O(\D t^2)$.
Write down the Kolmogorov forward and backward equations of $N$ and solve them.
Let $N_0 = 0$ and let $\tau_1$ be the time of the first jump of $N$.
If $\lambda(t) = c/(1 + t)$ show that $E(\tau_1) < \infty$ if and only if $c > 1$. \\

\noindent
\textit{Solution:} \\
Once again I think it is very helpful to write down the generator $\bm G$ for this scenario
\begin{align*}
\bm G = 
\begin{bmatrix}
- \lambda(t) & \lambda(t) & 0 & \dots \\
0 & - \lambda(t) & \lambda(t) & \ddots \\
\vdots & \ddots & \ddots & \ddots
\end{bmatrix}
\end{align*}
For the forward Kolmogorov equation we have
$$ \frac \D {\D t} \bm P_t = \bm P_t \bm G $$
which more explicitly is
\begin{align*}
\begin{bmatrix}
p^\prime_t(0,0) & p^\prime_t(0,1) & \dots \\
p^\prime_t(1,0) & p^\prime_t(1,1) & \dots \\
\vdots & \vdots & \ddots
\end{bmatrix}
=
\begin{bmatrix}
p_t(0,0) & p_t(0,1) & \dots \\
p_t(1,0) & p_t(1,1) & \dots \\
\vdots & \vdots & \ddots
\end{bmatrix}
\begin{bmatrix}
- \lambda(t) & \lambda(t) & 0 & \dots \\
0 & - \lambda(t) & \lambda(t) & \ddots \\
\vdots & \ddots & \ddots & \ddots
\end{bmatrix}.
\end{align*}
This results in the following system of differential equations
\begin{align*}
p^\prime_t(0,0) &= - \lambda(t) p_t(0,0) \\
p^\prime_t(0,1) &= \lambda(t) p_t(0,0) - \lambda(t) p_t(0,1) \\
	&\vdots \\
p^\prime_t(0,n) &= \lambda(t) p_t(0,n-1) - \lambda(t) p_t(0,n)
\end{align*}
Multiplying through by $s^n$ and summing over all $n$ we have
\begin{align*}
\sum_{n=0}^\infty s^n p^\prime_t(0,n)
	&= \lambda(t) \sum_{n=0}^\infty s^n p_t(0,n-1) - \lambda(t) \sum_{n=0}^\infty s^n p_t(0,n) \\
\frac {\D }{\D t} G_{N_t}(s)
	&= \lambda(t) s \sum_{n=0}^\infty s^{n - 1} p_t(0,n-1) - \lambda(t) G_{N_t}(s) \\
\frac {\D }{\D t} G_{N_t}(s)
	&= \lambda(t) s G_{N_t}(s) - \lambda(t) G_{N_t}(s) \\
\frac {\D }{\D t} G_{N_t}(s)
	&= (s  - 1) \lambda(t) G_{N_t}(s).
\end{align*}
Now we have the pde (again suppressing extra notation) $G_{N_0}(s) = s^0 = 1$
$$
\frac {\D }{\D t} G = (s  - 1) \lambda(t) G.
$$
Let's solve this directly
\begin{align*}
\frac {\D }{\D t} G &= (s  - 1) \lambda(t) G \\
\frac {\frac {\D }{\D t} G}{G} &= (s  - 1) \lambda(t) \\
\int \frac {\frac {\D }{\D t} G}{G} \D t &= \int (s  - 1) \lambda(t) \D t \\
\log \left( G \right) &= \int (s  - 1) \lambda(t) \D t \\
G &= \E^{\int (s  - 1) \lambda(t) \D t}.
\end{align*}
\textbf{TODO: You need to solve this!! Use the initial condition $N_0 = 0$ and the function given for $\lambda(t)$.}
For the backward Kolmogorov equation we have
$$ \frac \D {\D t} \bm P_t = \bm G \bm P_t $$
which more explicitly is
\begin{align*}
\begin{bmatrix}
p^\prime_t(0,0) & p^\prime_t(0,1) & \dots \\
p^\prime_t(1,0) & p^\prime_t(1,1) & \dots \\
\vdots & \vdots & \ddots
\end{bmatrix}
=
\begin{bmatrix}
- \lambda(t) & \lambda(t) & 0 & \dots \\
0 & - \lambda(t) & \lambda(t) & \ddots \\
\vdots & \ddots & \ddots & \ddots
\end{bmatrix}
\begin{bmatrix}
p_t(0,0) & p_t(0,1) & \dots \\
p_t(1,0) & p_t(1,1) & \dots \\
\vdots & \vdots & \ddots
\end{bmatrix}.
\end{align*}
This results in the following system of differential equations
\begin{align*}
p^\prime_t(0,0) &= - \lambda(t) p_t(0,0) + \lambda(t) p_t(1,0) \\
p^\prime_t(0,1) &= - \lambda(t) p_t(0,1) + \lambda(t) p_t(1,1) \\
p^\prime_t(0,2) &= - \lambda(t) p_t(0,2) + \lambda(t) p_t(1,2) \\
	& \vdots \\
p^\prime_t(0,n) &= - \lambda(t) p_t(0,n) + \lambda(t) p_t(1,n).
\end{align*}
We also have
\begin{align*}
p^\prime_t(1,0) &= - \lambda(t) p_t(1,0) + \lambda(t) p_t(2,0) \\
p^\prime_t(1,1) &= - \lambda(t) p_t(1,1) + \lambda(t) p_t(2,1) \\
p^\prime_t(1,2) &= - \lambda(t) p_t(1,2) + \lambda(t) p_t(2,2) \\
	& \vdots \\
p^\prime_t(1,n) &= - \lambda(t) p_t(1,n) + \lambda(t) p_t(1,n).
\end{align*}
I'm not totally sure how to combine these, however...
Multiplying through by $s^n$ and summing over all $n$ for the set of pde's where $N_0 = 0$
\begin{align*}
\sum_{n=0}^\infty s^n p^\prime_t(0,n)
	&= - \lambda(t) \sum_{n=0}^\infty s^n p_t(0,n) + \lambda(t)  \sum_{n=0}^\infty s^n p_t(1,n) \\
\frac {\partial}{\partial t} G_{N_t}(s)
	&= - \lambda(t) G_{N_t}(s) + \lambda(t)  \sum_{n=0}^\infty s^n p_t(1,n)
\end{align*}
How do I handle this thing with the incrementing in the first index not the second?
\newpage

\noindent {\bf 4.} Exercise 5.5.
Let $N$ be a poisson process with a random intensity $\Lambda$ witch is equal to $\lambda_1$ with probability $p$ and $\lambda_2$ with probability $1 - p$.
Find $G_{N_t}(s) = E(s^{N_t})$.
What is the mean and variance of $N_t$? \\

\noindent
\textit{Solution:} \\
\textbf{TODO:} \\
\begin{align*}
Here you
\end{align*}

\end{document}  
