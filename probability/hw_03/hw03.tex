\documentclass[10pt]{amsart}
\include{amsmath}
\usepackage{dsfont}													% gives you \mathds{} font
\usepackage{amssymb}
\usepackage{bbm}

\newcommand{\D}{\mathrm{d}}
\DeclareMathOperator{\E}{e}

\begin{document}

\noindent
\text{Hunter Lybbert} \\
\text{Student ID: 2426454} \\
\text{10-23-24} \\
\text{AMATH 561}
\title{Problem Set 3}
\maketitle

\noindent {\bf 1.} Give an example of a probability space $(\Omega, \mathcal{F},P)$, a random variable $X$ and a function $f$ such that $\sigma(f(X))$ is strictly smaller than $\sigma(X)$ but $\sigma(f(X)) \neq \{\emptyset,\Omega\}$. Give a function $g$ such that $\sigma(g(X))=\{\emptyset,\Omega\}$. Hint: Look at finite sample spaces with a small number of elements. \\
\textit{Solution:} \\
Let our probability space be two independent coin tosses, such that $\Omega = \big\{HH, TT, HT, TH\big\}$.
Define a random variable $X$ such that $X(\omega)$ be the number of heads in the outcome $\omega$ with $\omega \in \Omega$.
Therefore
\begin{align*}
X(HH) &= 2, \\
X(TT) &= 0, \\
X(HT) &= 1, \:\: \text{and}\\
X(TH) &= 1.
\end{align*}
Now $\sigma(X)$ can be written as
$$
\sigma(X) = \bigg\{ \{HH\}, \{TH, HT\}, \{TT\}, \{TT, HH\}, \{HH, HT, TH\}, \{TT, HT, TH\}, \Omega, \emptyset \bigg\}
$$
\\
\textbf{Part one} \\
Random variable $X$ and $f$ such that $\sigma(f(X)) \subsetneq \sigma(X)$ and $\sigma(f(X))$ is not the trivial $\sigma$-algebra. \\

\noindent
Define $f(x)$ as follows
$$
f(x)= \begin{cases}
 	0, & x \leq 0 \\
	1, & x > 0.
\end{cases}
$$
Let's look at the possible pre-images of $f(X)$ with respect to a few cases of Borel sets.
For convenience, I will define $\hat X = f(X)$.
Now let's look at some cases for the pre-image\\
\textit{Case 1:} $0 \in B$ but $1 \not\in B$
\begin{align*}
\hat X^{-1} (B) = \left\{ \omega : \hat X(\omega) \in B \right\} = \left\{ \omega : X(\omega) \in (-\infty, 0] \ \right\} = \{ TT \}
\end{align*}
\textit{Case 2:} $0 \not \in B$ but $1 \in B$
\begin{align*}
\hat X^{-1} (B) = \left\{ \omega : \hat X(\omega) \in B \right\} = \left\{ \omega : X(\omega) \in (0, \infty) \ \right\} = \{ TH, HT, HH \}
\end{align*}
\textit{Case 3:} $0 \in B$ and $1 \in B$
\begin{align*}
\hat X^{-1} (B) = \left\{ \omega : \hat X(\omega) \in B \right\} = \left\{ \omega : X(\omega) \in (-\infty, \infty) \ \right\} = \Omega
\end{align*}
\textit{Case 4:} $0 \not\in B$ and $1 \not\in B$
\begin{align*}
\hat X^{-1} (B) = \left\{ \omega : \hat X(\omega) \in B \right\} = \emptyset.
\end{align*}
Therefore,
\begin{align*}
\sigma(f(X)) = \bigg\{
	\{ TT \},
	\{ TH, HT, HH \},
	\Omega,
	\emptyset
\bigg\} \neq \left\{\emptyset, \Omega \right\}
\end{align*}
And thus we have $\sigma(f(X)) \subsetneq \sigma(X)$. \\
\qed \\
\textbf{Part two}
Now also give a function $g$ such that $\sigma(g(X))$ is the trivial $\sigma$-algebra, $\left\{ \emptyset, \Omega\right\}$.

\noindent
Define $g(x)$ to be a constant $c \in \mathbb R$ such that $g(x) = c$ for all $x \in \mathbb R$.
Once again, for convenience we define $\tilde X = g(X)$.
Let's go through a few cases of what the pre-image may be for any Borel set\\
\textit{Case 1:} $c \in B$
\begin{align*}
\tilde X^{-1} (B) = \left\{ \omega : \tilde X(\omega) \in B \right\} = \left\{ \omega : X(\omega) \in (-\infty, \infty) \ \right\} = \Omega
\end{align*}
\textit{Case 2:} $c \not \in B$
\begin{align*}
\tilde X^{-1} (B) = \left\{ \omega : \tilde X(\omega) \in B \right\} = \emptyset.
\end{align*}
Therefore,
\begin{align*}
\sigma(g(X)) = \left\{ \Omega, \emptyset \right\}.
\end{align*}
\qed
\\

\noindent {\bf 2.} Give an example of events $A$, $B$, and $C$, each of probability strictly between 0 and 1, such that
$P(A\cap B)=P(A)P(B), P(A\cap C)=P(A)P(C)$, and $P(A\cap B\cap C)=P(A)P(B)P(C)$ but $P(B\cap C)\neq P(B)P(C)$. Are $A$, $B$ and $C$ independent? Hint: You can let $\Omega$ be a set of eight equally likely points. \\
\textit{Solution:} \\
Let $\Omega = \{1, 2, 3, 4, 5, 6, 7, 8\}$.
Define events $A$, $B$, and $C$ as follows
\begin{align*}
A = \{1, 2, 3, 4\} \\
B = \{1, 2, 5, 7\} \\
C = \{1, 3, 6, 8\}.
\end{align*}
Then we have 
$$P(A \cap B) = P(\{ 1, 2 \}) = \frac 1 4$$
and
$$P(A)P(B) = P(\{1, 2, 3, 4\})P(\{1, 2, 5, 7\}) =  \frac 1 2 \cdot \frac 1 2 = \frac 1 4.$$
Additionally, we have 
$$P(A \cap C) = P(\{ 1, 3 \}) = \frac 1 4$$
and
$$P(A)P(C) = P(\{1, 2, 3, 4\})P(\{1, 3, 6, 8\}) =  \frac 1 2 \cdot \frac 1 2 = \frac 1 4.$$
Finally, we have 
$$P(A \cap B \cap C) = P(\{ 1\}) = \frac 1 8$$
and
$$P(A)P(B)P(C) = P(\{1, 2, 3, 4\})P(\{1, 2, 5, 7\})P(\{1, 3, 6, 8\}) =  \frac 1 2 \cdot \frac 1 2 \cdot \frac 1 2 = \frac 1 8.$$
Notice we also get
$$P(B \cap C) = P(\{ 1 \}) = \frac 1 8$$
which is not equal to
$$P(B)P(C) = P(\{1, 2, 5, 7\})P(\{1, 3, 6, 8\}) =  \frac 1 2 \cdot \frac 1 2 = \frac 1 4.$$
In class we said two events $E$ and $E^\prime$ are independent if $P(E\cap E^\prime) = P(E)P(E^\prime)$.
However, since independence of a collection of events $E_i$ for $i \in \{1, 2, 3, ..., n\}$ implies pairwise independence, $P(E_i\cap E_j) = P(E_i)P(E_j)$ for all $j \neq i$, if the collection $E_i$ fails to be pairwise independent then the collection must not be independent either.
We have shown that $A$ and $B$ are independent and $A$ and $C$ are independent.
But $B$ and $C$ are not independent, therefore we don't have pairwise independence between each pair of the three events hence $A$, $B$, and $C$ are not independent.
\qed
\\

\noindent {\bf 3.} Let $(\Omega, \mathcal{F},P)$ be a probability space such that $\Omega$ is countably infinite, and $\mathcal{F}=2^{\Omega}$. Show that it is impossible for there to exist a countable collection of events $A_1, A_2,... \in \mathcal{F}$ which are independent, such that $P(A_i)=1/2$ for each $i$. Hint: First show that for each $\omega \in \Omega$ and each $n\in \mathds{N}$, we have $P({\omega})\leq 1/2^n$. Then derive a contradiction. \\
\textit{Solution:} \\
Assume by way of contradiction, there exists a countably infinite collection of independent events $A_1, A_2, A_3, ... \in \mathcal F$ such that $P(A_i) = \frac 1 2$.
Independence of these events implies that
$$
P\left( \bigcap_i^n A_i\right) = \prod_i^n P(A_i) = \prod_i^n \frac 1 2 = \left(\frac 1 2\right)^n.
$$
I note that our collection of events is countably infinite so we can take the limit of the previous expression as $n\rightarrow\infty$.
Their independence also implies the independence of the events $A_i^c$, as discussed in class.
Next I want to construct a collection of new sets call them $B_{i,j}$ where $\omega_j \in B_{i,j}$ (note we can index the $\omega$'s since $\Omega$ is countably infinite).
Let $B_{i,j}$ be
$$
B_{i,j} = \begin{cases}
 	A_i, & \omega_j \in A_i \\
	A_i^c, & \omega_j \not \in A_i.
\end{cases}
$$
Therefore we can now write each $\omega_j$ as 
$$\bigcap_i^n B_{i,j} = \{\omega_j\}.$$
Then we have
$$ P(\{\omega_j\}) =  P\left( \bigcap_i^n B_{i,j} \right) = \prod_i^n P(B_{i,j}) = \prod_i^n \frac 1 2 = \left(\frac 1 2\right)^n = 0.$$
Where $P(\{ \omega_j \}) = 0$ since $n\rightarrow \infty$ because our collection of independent events is countably infinite.
Notice, since $\Omega = \cup_{j = 1}^{\infty} \{w_j\}$, then
$$
P(\Omega) = P(\cup_{j = 1}^{\infty} \{w_j\}) = \sum_{j = 1}^{\infty} P(\{ \omega_j\}) = \sum_{j = 1}^{\infty} 0 = 0.
$$
Which contradicts the fact that if $(\Omega, \mathcal{F},P)$ is a probability space then $P(\Omega) = 1$.
Therefore, it is impossible for there to exist a countable collection of events $A_1, A_2,... \in \mathcal{F}$ which are independent, such that $P(A_i)=1/2$ for each $i$.
\qed \\

\noindent {\bf 4.}  (a) Let $X \geq 0$ and $Y \geq 0$  be independent random variables with distribution functions $F$ and $G$. Find the distribution function of $XY$. \\
\textit{Solution:} \\
Let $h(x, y) = \mathbbm{1}_{\{xy \leq z\}}$ and $\mathbb E \left[ h(x,y) \right]$ be
$$
\mathbb E \left[ h(x,y) \right] = \int_{\mathbb R} \int_{\mathbb R} h(x, y) \mu(\D x) \nu(\D y)
$$
where $\mu$ and $\nu$ are probability measures with distribution functions $F$ and $G$ respectively.
We also have 
\begin{align*}
\mathbb E \left[ h(x,y) \right]
	&= \mathbb E \left[ \mathbbm{1}_{\{xy \leq z\}} \right] \\
	&= 1 \cdot P(XY \leq z) + 0 \cdot P(XY > 0) \\
	&= P(XY \leq z).
\end{align*}
Additionally,
\begin{align*}
\int_{\mathbb R} h(x, y) \mu(\D x) 
	&= \int_{\mathbb R} \mathbbm{1}_{\{xy \leq z\}} \mu(\D x) \\
	&= \int_{\mathbb R} \mathbbm{1}_{\{x \leq \frac z y\}} \mu(\D x) \\
	&= P\left( X \leq \frac z y \right) \\
	&= F\left(\frac z y \right).
\end{align*}
Combining these we have
\begin{align*}
P\left( XY \leq z \right)
	&= \mathbb E \left[ h(x,y) \right] \\
	&= \int_{\mathbb R} \int_{\mathbb R} h(x, y) \mu(\D x) \nu(\D y) \\
	&= \int_{\mathbb R} F\left(\frac z y \right) \nu(\D y) \\
	&= \int_{\mathbb R} F\left(\frac z y \right) \D G(y)
\end{align*}
\textbf{TODO:} Account for when y is 0 somehow...
End up with $P(Y = 0)$ which is $G(0)$
\begin{align*}
\int_{\mathbb R} F\left(\frac z y \right) \D G(y)
	&= \int_{-\infty}^0 F\left(\frac z y \right) \D G(y) + \int_{0}^{\infty} F\left(\frac z y \right) \D G(y).
\end{align*}
\\

(b) If $X \geq 0$ and $Y \geq 0$ are independent continuous random variables with density functions $f$ and $g$, find the density function of $XY$. \\
\textit{Solution:} \\
\begin{align*}
\int_{\mathbb R} F\left(\frac z y \right) \D G(y) = \int_{\mathbb R} \int_{-\infty}^{\frac z y} f\left(u \right) \D u \D G(y).
\end{align*}
Now we need to do something with a change of variables along the lines of $u = \frac x y$ then $\D u = \frac{\D x}{y}$. We have
\begin{align*}
\int_{\mathbb R} F\left(\frac z y \right) \D G(y)
	&= \int_{\mathbb R} \int_{-\infty}^{\frac z y} f\left(u \right) \D u \D G(y) \\
	&= \int_{\mathbb R} \int_{-\infty}^z \frac {1}{y}f\left(\frac x y \right) \D x \D G(y) \\
	&= \int_{-\infty}^z \int_{\mathbb R} \frac {1}{y}f\left(\frac x y \right) \D G(y)\D x \\
	&= P\left( XY \leq z \right).
\end{align*}
Therefore the density is
$$
f(x) = \int_{\mathbb R} \frac {1}{y}f\left(\frac x y \right) \D G(y).
$$
Since $Y$ has a density $g$, we can write the above as
$$
f(x) = \int_{\mathbb R} \frac {1}{y}f\left(\frac x y \right)g(y)\D y
$$
using Theorem 3 from Lecture 9 slides.
\qed \\

(c) If $X$ and $Y$ are independent exponentially distributed random variables with parameter $\lambda$, find the density function of $XY$.\\
\textit{Solution:} \\
Recall the density function of an exponentially distributed random variable with parameter $\lambda$ is the same as a gamma distributed random variable with parameters $(1, \lambda)$.
Therefore the density function for one of our random variables is
$$
f(x) = \frac {\lambda^1} {\Gamma(1)} x^{1 - 1} \E^{-\lambda x} = \lambda \E^{-\lambda x}.
$$
Now using the formula we have for the density of the product of two independent random variables we have
\begin{align*}
f(x) &= \int_{\mathbb R} \frac {1}{y}f\left(\frac x y \right)g(y)\D y \\
	&= \int_{\mathbb R} \frac {1}{y} \lambda \E^{-\lambda \frac x y} \lambda \E^{-\lambda y} \D y \\
	&= \int_{\mathbb R} \frac {1}{y} \lambda^2 \E^{-\lambda \left( \frac x y + y \right)} \D y.
\end{align*}
\textbf{TODO:} Consider clarifying the bounds of all of your integration (so it matches the support of $X$, $Y$)!
\\
\end{document}  
